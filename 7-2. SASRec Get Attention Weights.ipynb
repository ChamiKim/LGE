{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50112af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model.sasrec import SASRecModel\n",
    "from trainers import Trainer\n",
    "from utils import EarlyStopping, check_path, set_seed, set_logger\n",
    "from dataset import get_seq_dic, get_dataloder, get_rating_matrix\n",
    "\n",
    "# Set up arguments\n",
    "class Args:\n",
    "    data_dir = \"./data/\"\n",
    "    output_dir = \"output/\"\n",
    "    data_name = \"input_search_augmented_final\"\n",
    "    do_eval = False\n",
    "    load_model = None\n",
    "    train_name = \"test_model\"\n",
    "    num_items = 10\n",
    "    num_users = 10\n",
    "    lr = 0.001\n",
    "    batch_size = 256\n",
    "    epochs = 10\n",
    "    no_cuda = False\n",
    "    log_freq = 1\n",
    "    patience = 2\n",
    "    num_workers = 0  # Set num_workers to 0 to avoid BrokenPipeError on Windows\n",
    "    seed = 42\n",
    "    weight_decay = 0.0\n",
    "    adam_beta1 = 0.9\n",
    "    adam_beta2 = 0.999\n",
    "    gpu_id = \"0\"\n",
    "    variance = 5\n",
    "    model_type = 'bert4rec'\n",
    "#     model_type = 'sasrec_model'\n",
    "    max_seq_length = 50\n",
    "    hidden_size = 256\n",
    "    num_hidden_layers = 2\n",
    "    hidden_act = \"gelu\"\n",
    "    num_attention_heads = 2\n",
    "    attention_probs_dropout_prob = 0.5\n",
    "    hidden_dropout_prob = 0.5\n",
    "    initializer_range = 0.02\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/data/log-data-2024/SASRec/BSARec/src/data/input_search_sample_matching_final.txt\")\n",
    "samp = list(pd.read_csv(\"/data/log-data-2024/8man_sample_new.csv\")[\"treatment1\"])\n",
    "\n",
    "df[\"live\"] = [1 if i in samp else 0 for i in tqdm(df[\"device_id\"])]\n",
    "df = df[df[\"live\"] == 1].reset_index(drop = True)\n",
    "\n",
    "print(list(df[\"device_id\"]) == samp)\n",
    "input_data = df[\"sequence\"]\n",
    "\n",
    "input_data = [\" \".join(i.split()[1:]) for i in input_data]\n",
    "    \n",
    "input_ids = []\n",
    "for line in input_data:\n",
    "    items = list(map(int, line.strip().split()))\n",
    "    pad_len = args.max_seq_length - len(items)\n",
    "    input_ids.append([0] * pad_len + items)\n",
    "    \n",
    "# get attention weights\n",
    "model = torch.load(\"bert4rec.pt\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "input_ids = torch.tensor(input_ids).to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    extended_attention_mask = model.get_attention_mask(input_ids)\n",
    "    sequence_emb = model.add_position_embedding(input_ids)\n",
    "    item_encoded_layers = model.item_encoder(sequence_emb, extended_attention_mask, output_all_encoded_layers=True)\n",
    "    attention_weights = item_encoded_layers[-1]  # Extract the attention weights from the last layer\n",
    "\n",
    "mean_attention_weights = torch.mean(attention_weights, dim=-1)\n",
    "print(mean_attention_weights.size())\n",
    "attention_probs = torch.nn.functional.softmax(mean_attention_weights, dim=-1)\n",
    "\n",
    "attention_probs = attention_probs.tolist()\n",
    "input_ids = input_ids.tolist()\n",
    "\n",
    "ap = []\n",
    "ii = []\n",
    "for a, i in enumerate(input_ids):\n",
    "    while 0 in i:\n",
    "        i.remove(0)\n",
    "    ap.append(attention_probs[a][len(i) * (-1):])\n",
    "    ii.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "focus = []    \n",
    "for a, i in enumerate(ap):\n",
    "    first_max_index = np.argmax(i)\n",
    "    i[first_max_index] = -np.inf\n",
    "    second_max_index = np.argmax(i)\n",
    "    focus.append([ii[a][first_max_index], ii[a][second_max_index]])\n",
    "    \n",
    "with open(file='/data/log-data-2024/SASRec/BSARec/src/SASRec_prediction_attention.pickle', mode='wb') as f:\n",
    "    pickle.dump(focus, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
